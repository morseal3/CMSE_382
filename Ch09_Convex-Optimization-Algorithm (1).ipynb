{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization over a Convex Set\n",
    "\n",
    "We consider the constrained optimiztion problem \n",
    "$$\\min_{\\mathbf{x}}f(\\mathbf{x})\\quad\\mbox{ s.t. } \\mathbf{x}\\in C,$$\n",
    "where $C\\in\\mathbb{R}^n$ is a closed convex set and $f:C\\rightarrow R$ is continuously differentiable. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuitive Example\n",
    "\n",
    "$$\\min_x~(x-1)^2\\quad\\mbox{ s.t. } x\\in [2,3]$$\n",
    "\n",
    "Its optimal solution is $x^*=2$. However, if we just apply gradient descent to the objective function, it is very difficulty to get exact $2$. In fact, gradient descent will end up $x=1$, which is not feasible (because $x\\notin [2,3]$). One way to solve this problem can be projected gradient descent. After each gradient descent step, we project to the feasible set. For this problem, whenever we get a point smaller than 2, we can project it to 2. Then we start at 2 and we will not leave 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity\n",
    "\n",
    "**Recall.** When $C=\\mathbb{R}^n$, the point $\\mathbf{x}^*$ is a stationary point if $\\nabla f(\\mathbf{x}^*)=\\mathbf{0}$.\n",
    "\n",
    "**Definition.** A point $\\mathbf{x}^*\\in C$ is a **stationary point** if $\\nabla f(\\mathbf{x}^*)^\\top(\\mathbf{x}-\\mathbf{x}^*)\\geq 0$ for all $\\mathbf{x}\\in C$. \n",
    "\n",
    "This definition is also true when $C=\\mathbb{R}^n$, in which case, $\\nabla f(\\mathbf{x}^*)=\\mathbf{0}$.\n",
    "\n",
    "It means that $\\mathbf{x}^*$ is a stationary point if and only if there is no feasible descent direction of $f$ at $\\mathbf{x}^*$.\n",
    "\n",
    "**Theorem.** If $\\mathbf{x}^*$ is a local minimizer, then $\\mathbf{x}^*$ is a stationary point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example ($C=\\mathbf{R}_+^n$).** \n",
    "$${\\partial\\over\\partial x_i}f(\\mathbf{x}^*)\\left\\{\\begin{matrix}=0, & x_i^*>0,\\\\\\geq 0, & x_i^*=0.\\end{matrix}\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example ($C=\\{\\mathbf{x}:\\mathbf{e}^\\top\\mathbf{x}=1\\}$).** \n",
    "$${\\partial\\over\\partial x_1}f(\\mathbf{x}^*)={\\partial\\over\\partial x_2}f(\\mathbf{x}^*)=\\cdots={\\partial\\over\\partial x_n}f(\\mathbf{x}^*).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example ($C=\\{\\mathbf{x}:\\|\\mathbf{x}\\|\\leq1\\}$).** \n",
    "\n",
    "If $\\nabla f(\\mathbf{x}^*)^\\top(\\mathbf{x}-\\mathbf{x}^*)\\geq 0$ for all $\\mathbf{x}\\in C$. We have \n",
    "$$\\min_{\\|\\mathbf{x}\\|\\leq1} \\nabla f(\\mathbf{x}^*)^\\top(\\mathbf{x}-\\mathbf{x}^*)\\geq 0$$\n",
    "So we have \n",
    "\\begin{align*}\n",
    "0\\leq \\min_{\\|\\mathbf{x}\\|\\leq1} \\nabla f(\\mathbf{x}^*)^\\top(\\mathbf{x}-\\mathbf{x}^*)=\\min_{\\|\\mathbf{x}\\|\\leq1} \\nabla f(\\mathbf{x}^*)^\\top\\mathbf{x}-\\nabla f(\\mathbf{x}^*)^\\top\\mathbf{x}^* = -\\|\\nabla f(\\mathbf{x}^*)\\|-\\nabla f(\\mathbf{x}^*)^\\top\\mathbf{x}^* \n",
    "\\end{align*}\n",
    "Therefore, we need \n",
    "\\begin{align*}\n",
    "-\\|\\nabla f(\\mathbf{x}^*)\\|\\geq \\nabla f(\\mathbf{x}^*)^\\top\\mathbf{x}^*\\geq -\\|\\nabla f(\\mathbf{x}^*)\\|\n",
    "\\end{align*}\n",
    "and \n",
    "$\\nabla f(\\mathbf{x}^*)=-\\lambda\\mathbf{x}^*$ with $\\lambda\\leq 0$.\n",
    "\n",
    "The overall condition here is that $\\nabla f(\\mathbf{x}^*)=0$ or $||\\mathbf{x}^*||=1$ and there exists $\\lambda\\leq 0$ such that $\\nabla f(\\mathbf{x}^*)=\\lambda\\mathbf{x}^*$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convex problems\n",
    "We will assume that $f$ is convex in this part.\n",
    "\n",
    "**Theorem.** $\\mathbf{x}^*$ is a stationary point if and only if $\\mathbf{x}^*$ is an optimal solution. \n",
    "\n",
    "**Proof (only if).** \n",
    "$$f(\\mathbf{x})\\geq f(\\mathbf{x}^*)+\\nabla f(\\mathbf{x}^*)^\\top(\\mathbf{x}-\\mathbf{x}^*)\\geq f(\\mathbf{x}^*).$$\n",
    "\n",
    "**Proof (if).** \n",
    "If $\\mathbf{x}^*$ is an optimal solution, then we can not find a feasible descent direction at $\\mathbf{x}$, which means that $\\mathbf{x}^*$ is a stationary point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal projection\n",
    "\n",
    "Let $P_C(\\mathbf{x})$ be the projection of $\\mathbf{x}$ onto the closed convex set $C$.\n",
    "\n",
    "**Results.** \n",
    "+ $$(\\mathbf{x}-P_C(\\mathbf{x}))^\\top(\\mathbf{y}-P_C(\\mathbf{x}))\\leq 0\\quad\\forall \\mathbf{y}\\in C.$$\n",
    "+ $$\\|P_C(\\mathbf{x})-P_C(\\mathbf{y})\\|^2\\leq(P_C(\\mathbf{x})-P_C(\\mathbf{y})^\\top(\\mathbf{x}-\\mathbf{y}).$$\n",
    "**Proof.** For any $\\mathbf{y}$, we have $(\\mathbf{x}-P_C(\\mathbf{x}))^\\top(P_C(\\mathbf{y})-P_C(\\mathbf{x}))\\leq 0$. Switching the order of $\\mathbf{x}$ and $\\mathbf{y}$, we have $(\\mathbf{y}-P_C(\\mathbf{y}))^\\top(P_C(\\mathbf{x})-P_C(\\mathbf{y}))\\leq 0$. Combine both inequality, and we have \n",
    "$$\\left(\\mathbf{x}-\\mathbf{y}+P_C(\\mathbf{y})-P_C(\\mathbf{x})\\right)^\\top(P_C(\\mathbf{y})-P_C(\\mathbf{x}))\\leq 0.$$\n",
    "+ $$\\|P_C(\\mathbf{x})-P_C(\\mathbf{y})\\|\\leq \\|\\mathbf{x}-\\mathbf{y}\\|$$\n",
    "+ $\\mathbf{x}^*$ is a stationary point if and only if $$\\mathbf{x}^*=P_C(\\mathbf{x}^*-\\eta\\nabla f(\\mathbf{x}^*)).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The projected gradient (forward-backward) method\n",
    "\n",
    "$$\\mathbf{x}^*=P_C(\\mathbf{x}^*-\\eta\\nabla f(\\mathbf{x}^*))$$\n",
    "\n",
    "+ Input: $\\epsilon$, $\\mathbf{x}_0$\n",
    "    + Pick a stepsize $t_k$\n",
    "    + Update $\\mathbf{x}$ as $\\mathbf{x}_{k+1}=P_C(\\mathbf{x}_k-t_k\\nabla f(\\mathbf{x}_k)$\n",
    "    + Check $\\|\\mathbf{x}_k-\\mathbf{x}_{k+1}\\|\\leq \\epsilon$\n",
    "    \n",
    "### **Sufficient descrease.** \n",
    "\n",
    "**Recall.** If $f$ is $L$-smooth, then we have \n",
    "$$f(\\mathbf{x})-f(\\mathbf{x}-t\\nabla f(\\mathbf{x}))\\geq t\\left(1-{Lt\\over2}\\right)\\|\\nabla f(\\mathbf{x})\\|^2.$$\n",
    "\n",
    "**Theorem.** With the constraint $\\mathbf{x}\\in C$, we have a similar result.\n",
    "$$f(\\mathbf{x})-f(P_C(\\mathbf{x}-t\\nabla f(\\mathbf{x})))\\geq t\\left(1-{Lt\\over2}\\right)\\left\\|{1\\over t}(\\mathbf{x}-P_C(\\mathbf{x}-t\\nabla f(\\mathbf{x})))\\right\\|^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtracking line search\n",
    "\n",
    "**Recall.** \n",
    "$$f(\\mathbf{x}_k)-f(\\mathbf{x}_k-t_k\\nabla f(\\mathbf{x}_k))\\geq \\alpha t_k\\|\\nabla f(\\mathbf{x}_k)\\|^2$$\n",
    "\n",
    "In the constrained case, \n",
    "$$f(\\mathbf{x}_k)-f(P_C(\\mathbf{x}_k-t_k\\nabla f(\\mathbf{x}_k)))\\geq \\alpha t_k\\left\\|{1\\over t_k}(\\mathbf{x}-P_C(\\mathbf{x}-t_k\\nabla f(\\mathbf{x}_k))\\right\\|^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The convex case\n",
    "\n",
    "\n",
    "\n",
    "The function $f$ is $L$-smooth, then\n",
    "$$f(\\mathbf{x}_{k+1})\\leq f(\\mathbf{x}_k)+\\langle \\nabla f(\\mathbf{x}_k),\\mathbf{x}_{k+1}-\\mathbf{x}_k\\rangle+{L\\over 2}\\|\\mathbf{x}_k-\\mathbf{x}_{k+1}\\|^2$$\n",
    "If the function $f$ is convex, we have $$f(\\mathbf{x}_{k})\\leq f(\\mathbf{x}^*)+\\langle \\nabla f(\\mathbf{x}_k),\\mathbf{x}_{k}-\\mathbf{x}^*\\rangle.$$\n",
    "Combine them and we have \n",
    "$$f(\\mathbf{x}_{k+1})\\leq f(\\mathbf{x}^*)+\\langle \\nabla f(\\mathbf{x}_k),\\mathbf{x}_{k+1}-\\mathbf{x}^*\\rangle+{L\\over 2}\\|\\mathbf{x}_k-\\mathbf{x}_{k+1}\\|^2$$\n",
    "From the projection, we have \n",
    "$$\\langle \\mathbf{x}_k-t\\nabla f(\\mathbf{x}_k)-\\mathbf{x}_{k+1},\\mathbf{x}^*-\\mathbf{x}_{k+1}\\rangle \\leq 0.$$\n",
    "So we have \n",
    "$$\\begin{align*}\n",
    "f(\\mathbf{x}_{k+1})\\leq & f(\\mathbf{x}^*)+{1\\over t}\\langle\\mathbf{x}_k-\\mathbf{x}_{k+1},\\mathbf{x}_{k+1}-\\mathbf{x}^*\\rangle+{L\\over 2}\\|\\mathbf{x}_k-\\mathbf{x}_{k+1}\\|^2\\\\\n",
    "\\leq&f(\\mathbf{x}^*)+{1\\over t}\\langle\\mathbf{x}_k-\\mathbf{x}_{k+1},\\mathbf{x}_{k}-\\mathbf{x}^*\\rangle+{1\\over 2t}\\|\\mathbf{x}_k-\\mathbf{x}_{k+1}\\|^2\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Iterative Hard-Thresholding Method\n",
    "\n",
    "$$\\min\\limits_{\\mathbf{x}}~{\\|\\mathbf{A}\\mathbf{x}-\\mathbf{b}\\|^2}\\quad\\mbox{ s.t. } \\|\\mathbf{x}\\|_0\\leq s.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iht(x, A, b, s, eta):\n",
    "    M = 100\n",
    "    for i in range(M):\n",
    "        x = x - 2*eta*A.T@(A@x-b)\n",
    "        sortind = np.argsort(np.abs(x), axis=0)[::-1]\n",
    "        x[sortind[s:]] = 0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
